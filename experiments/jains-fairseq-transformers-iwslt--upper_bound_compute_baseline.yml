## NOTE:: We use --no-epoch-checkpoints and --save-dir $$PT_OUTPUT_DIR/checkpoints
# to make sure intermediate ckpts are saved.

description: jains-fairseq-transformers-iwslt--upper_bound_compute_baseline

target:
  cluster: rr1
  vc: resrchvc
  # cluster: eu1
  # vc: msrlabs
environment:
  image: pytorch/pytorch:1.4-cuda10.1-cudnn7-devel

storage:
  shawn:
    storage_account_name: hpalangivm0
    container_name: shawn
    local_dir: /data/home/jains/shawn

code:
  local_dir: /data/home/jains/Documents/fairseq

search:
  job_template:
    name: jains-fairseq-transformers-iwslt--upper_bound_compute_baseline--__encoder_experiment_layer_idx_{encoder_experiment_layer_idx}__layer_type_{layer_type}__k_{k}__encoder-ffn-embed-dim_{baseline_size}__encoder-embed-dim_{baseline_size}
    sku: G1
    sku_count: 1
    command:
    # - git clone https://github.com/pytorch/fairseq
    # - cd fairseq
    # - pip install --editable ./
    - pip install --user ./
    - pip install --user pyarrow
    - pip install --user fastBPE sacremoses subword_nmt
    - pip install --user tensorboardX
    - pip freeze > img_pip_freeze.txt
    - export TRAINBIN=$$(which fairseq-train)
    - echo TRAINBIN
    - echo $$TRAINBIN
    # Hack to get fairseq-* CLI tools on PATH, may be caused by --user install?
    - export PATH=$$PATH:/root/.local/bin:/home/jains/.local/bin
    - export TRAINBIN=$$(which fairseq-train)
    - echo TRAINBIN
    - echo $$TRAINBIN
    - >-
      python -W ignore $$(which fairseq-train)
      data-bin/iwslt14.tokenized.de-en
      --arch transformer_iwslt_de_en_JAIN
      --encoder_experiment_layer_idx {encoder_experiment_layer_idx}
      --encoder_experiment_layer_type {layer_type}
      --k {k}
      --share-decoder-input-output-embed
      --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0
      --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000
      --dropout 0.3 --weight-decay 0.0001
      --criterion label_smoothed_cross_entropy --label-smoothing 0.1
      --max-tokens 4096
      --eval-bleu
      --eval-bleu-detok moses
      --eval-bleu-remove-bpe
      --eval-bleu-print-samples
      --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
      --tensorboard-logdir {tensorboard_dir}
      --max-epoch 120
      --encoder-ffn-embed-dim {baseline_size}
      --encoder-embed-dim {baseline_size}
      --no-epoch-checkpoints
      --save-dir $$PT_OUTPUT_DIR/checkpoints
    # - cp checkpoints/checkpoint_best.pt $$PT_OUTPUT_DIR
    - >-
      fairseq-generate data-bin/iwslt14.tokenized.de-en
      --path $$PT_OUTPUT_DIR/checkpoints/checkpoint_best.pt
      --batch-size 128 --beam 5 --remove-bpe
      --tensorboard-logdir {tensorboard_dir}
      --results-path $$PT_OUTPUT_DIR/results_fairseq-generate-origmodel
    - echo "results_fairseq-generate-origmodel:::"
    - tail $$PT_OUTPUT_DIR/results_fairseq-generate-origmodel/*
    - >-
      fairseq-generate data-bin/iwslt14.tokenized.de-en
      --path $$PT_OUTPUT_DIR/checkpoints/checkpoint_best.pt
      --batch-size 128 --beam 5 --remove-bpe
      --tensorboard-logdir {tensorboard_dir}
      --results-path $$PT_OUTPUT_DIR/results_fairseq-generate-modifiedmodel
      {model_eval_overrides}
    - echo "results_fairseq-generate-modifiedmodel:::"
    - tail $$PT_OUTPUT_DIR/results_fairseq-generate-modifiedmodel/*
    - bash
    submit_args:
      container_args:
        shm_size: 128G
  type: grid
  max_trials: 100
  params:
    - name: tensorboard_dir
      spec: discrete
      values: ['$$PT_OUTPUT_DIR/logs']
    - name: encoder_experiment_layer_idx
      spec: discrete
      values: ['0,1,2,3,4,5']
    - name: layer_type
      spec: discrete
      values: ['crs --strategy det_top_k']
    - name: k
      spec: discrete
      values: ['0']
    - name: baseline_size
      spec: discrete
      values: ['64', '256']
    - name: model_eval_overrides
      spec: discrete
      values: ['--model-overrides "{''k'': 0}"']

# --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' \
