description: jains-fairseq-transformersALPHA-iwslt-wide-baselines-width-depth-sweeps-experiments

target:
  cluster: rr1
  vc: resrchvc
  # cluster: eu1
  # vc: msrlabs
environment:
  image: pytorch/pytorch:1.4-cuda10.1-cudnn7-devel

storage:
  shawn:
    storage_account_name: hpalangivm0
    container_name: shawn
    local_dir: /data/home/jains/shawn

code:
  local_dir: /data/home/jains/Documents/fairseq

search:
  job_template:
    name: wide-experiments__width_{model_width}__n_layers_{n_layers}__fp16_{fp16}__max_tokens_{max_tokens}__encoder_experiment_layer_idx_{encoder_experiment_layer_idx}__layer_type_{layer_type}__k_{k}__runnumber_{run_number}
    sku: G1
    sku_count: 1
    command:
    # - git clone https://github.com/pytorch/fairseq
    # - cd fairseq
    # - pip install --editable ./
    # - pip install ipdb
    - pip install --user ./
    - pip install --user pyarrow
    - pip install --user fastBPE sacremoses subword_nmt
    - pip install --user sacrebleu==1.4.10
    - pip install --user tensorboardX
    - pip freeze > img_pip_freeze.txt
    # Hack to get fairseq-* CLI tools on PATH, may be caused by --user install?
    - export PATH=$$PATH:/root/.local/bin:/home/jains/.local/bin
    - echo RUN_NUMBER {run_number}
    - >-
      python -W ignore $$(which fairseq-train)
      data-bin/iwslt14.tokenized.de-en
      --arch transformer_alpha_iwslt_de_en_JAIN
      --encoder_experiment_layer_idx {encoder_experiment_layer_idx}
      --encoder_experiment_layer_type {layer_type}
      --k {k}
      --share-decoder-input-output-embed
      --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0
      --lr-scheduler inverse_sqrt --warmup-updates 4000
      --dropout 0.3 --weight-decay 0.0001
      --criterion label_smoothed_cross_entropy --label-smoothing 0.1
      --eval-bleu
      --eval-bleu-detok moses
      --eval-bleu-remove-bpe
      --eval-bleu-print-samples
      --tensorboard-logdir {tensorboard_dir}
      --max-epoch 120
      --no-epoch-checkpoints
      --save-dir $$PT_OUTPUT_DIR/checkpoints
      --max-tokens {max_tokens}
      {model_width}
      --encoder-layers {n_layers} --decoder-layers {n_layers}
      --user-dir alpha_lang/code/alpha_lang
      --last-layer-alpha 1
      --attn-alpha 4
      {fp16}
      --scale-after-matmal
    - >-
      fairseq-generate data-bin/iwslt14.tokenized.de-en
      --path $$PT_OUTPUT_DIR/checkpoints/checkpoint_best.pt
      --batch-size 128 --beam 5 --remove-bpe
      --tensorboard-logdir {tensorboard_dir}
      --results-path $$PT_OUTPUT_DIR/results_fairseq-generate-origmodel
      --user-dir alpha_lang/code/alpha_lang
    - echo "results_fairseq-generate:::"
    - tail $$PT_OUTPUT_DIR/results_fairseq-generate-origmodel/*
    - >-
      fairseq-generate data-bin/iwslt14.tokenized.de-en
      --path $$PT_OUTPUT_DIR/checkpoints/checkpoint_best.pt
      --batch-size 128 --beam 5 --remove-bpe
      --tensorboard-logdir {tensorboard_dir}
      --results-path $$PT_OUTPUT_DIR/results_fairseq-generate-modifiedmodel
      --user-dir alpha_lang/code/alpha_lang
      {model_eval_overrides}
    - echo "results_fairseq-generate-modifiedmodel:::"
    - tail $$PT_OUTPUT_DIR/results_fairseq-generate-modifiedmodel/*
    submit_args:
      container_args:
        shm_size: 128G
  type: grid
  max_trials: 256
  params:
    - name: tensorboard_dir
      spec: discrete
      values: ['$$PT_OUTPUT_DIR/logs']
    - name: model_width
      spec: discrete
      values: [
        # '--lr 1.00e-03 --encoder-ffn-embed-dim 256 --encoder-embed-dim 128 --decoder-ffn-embed-dim 256 --decoder-embed-dim 128 --encoder-normalize-before --decoder-normalize-before',  # VNARROW width
        # '--lr 7.07e-04 --encoder-ffn-embed-dim 512 --encoder-embed-dim 256 --decoder-ffn-embed-dim 512 --decoder-embed-dim 256 --encoder-normalize-before --decoder-normalize-before',  # NARROW width
        # '--lr 5e-4 --encoder-normalize-before --decoder-normalize-before',  # STANDARD width
        # '--lr 0.00035355339059327376 --encoder-ffn-embed-dim 2048 --encoder-embed-dim 1024 --decoder-ffn-embed-dim 2048 --decoder-embed-dim 1024 --encoder-normalize-before --decoder-normalize-before  --update-freq 4',  # MED # --update-freq 4 allows us to have an effective batch size of 4*--max-tokens
        # '--lr 0.00025 --encoder-ffn-embed-dim 4096 --encoder-embed-dim 2048 --decoder-ffn-embed-dim 4096 --decoder-embed-dim 2048',  # WIDE
        '--lr 0.00025 --encoder-ffn-embed-dim 4096 --encoder-embed-dim 2048 --decoder-ffn-embed-dim 4096 --decoder-embed-dim 2048 --update-freq 4',  # WIDE  # --update-freq 4 allows us to have an effective batch size of 4*--max-tokens
      ]
    - name: n_layers
      spec: discrete
      values: [
        '1',
        '2',
        '3',
        '4',
        '5',
        '6',
        # '7',
        # '8',
        # '9',
        # '10',
        # '20',
        # '30',
      ]
    - name: run_number
      spec: discrete
      values: [
        '0',
        '1',
      ]
    - name: max_tokens
      spec: discrete
      values: [
        # '8192',  # WIDE, OOM at epoch 1, 76%, inital alloc 15.3GB of 16.3GB
        # '4096',  # WIDE, inital alloc 9GB
        '2048',
        # '1024',
        # '512',
      ]
    - name: fp16
      spec: discrete
      values: [
        '--fp16',
        # '--memory-efficient-fp16'
      ]
    - name: encoder_experiment_layer_idx
      spec: discrete
      values: ['-1']
    - name: layer_type
      spec: discrete
      values: ['crs --strategy det_top_k', 'crs --strategy single_norm']
      # values: ['crs --strategy single_norm']
      # values: ['meProp_unified', 'shawn_unified', 'crs --strategy det_top_k', 'crs --strategy first_k', 'crs --strategy nps']
    - name: k
      spec: discrete
      values: [
        '64',
        '256'
      ]
    - name: model_eval_overrides
      spec: discrete
      values: ['--model-overrides "{''k'': 0}"']
# Add this back into the train args to report and save checkpoints based on best_bleu instead of best_loss
# --best-checkpoint-metric bleu --maximize-best-checkpoint-metric

# --fp16
# 8192, wide, 10 layers -- fails
# 8192, wide, 1 layers -- fails
# 1024, wide, 5 layers -- runs!
# 2048, wide, 5 layers -- runs!
# 2048, wide, 6 layers -- runs!
# 4096, wide, 5 layers -- runs, but likely to OOM.
# 4096, wide, 6 layers -- runs, but likely to OOM.
# 1024, wide, 10 layers -- fails
# 1024, wide, 6 layers -- runs!
# 512, wide, 6 layers -- runs!
# 8192, wide, 6 layers -- fails
# 8192, standard, 40 layers -- fails
# 8192, standard, 50 layers -- fails
# 8192, MED, 30 layers -- fails
# 2048, MED, 30 layers -- fails
# 8192, MED, 20 layers -- fails
# 1024, MED, 20 layers -- works!
# 2048, MED, 20 layers -- works!
# 8192, NARROW, 30 layers -- works!



# '--lr 0.00025 --encoder-ffn-embed-dim 4096 --encoder-embed-dim 2048 --decoder-ffn-embed-dim 4096 --decoder-embed-dim 2048 --update-freq 4',  # WIDE  # --update-freq 4 allows us to have an effective batch size of 4*--max-tokens
# train_wall 226

# '--lr 0.00025 --encoder-ffn-embed-dim 4096 --encoder-embed-dim 2048 --decoder-ffn-embed-dim 4096 --decoder-embed-dim 2048',  # WIDE  # --update-freq 4 allows us to have an effective batch size of 4*--max-tokens
# train_wall 34
# 6x higher train_wall time!
